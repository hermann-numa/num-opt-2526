{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "1c2230bf-18c1-40c2-8a6a-5c1d86705948",
      "cell_type": "markdown",
      "source": "# Programmieraufgabe: CG-Verfahren und PCG-Verfahren\n\n<span style=\"color:red\">Die bisherige Version von Programmieraufgabe 6 war leider missverständlich gestellt. Sie dürfen gerne diese aktualisierte Version benutzen.</span>\n\n<span style=\"color:red\">Änderungen:</span>\n* <span style=\"color:red\">Sie dürfen eine vereinfachte Variante der Matrix $A$ benutzen.</span>\n* <span style=\"color:red\">Die Methoden `cg` und `pcg` wurden an das Skript angepasst.</span>\n\nWie in Beispiel 3.39 im Skript betrachten wir ein quadratisches Optimierungsproblem, das aus der zweidimensionalen Wärmeleitungsgleichung resultiert. Dabei hat die Matrix $A$ die Form\n$$\n A = \\begin{bmatrix}\n    B  & C     &        &    \\\\\n    C^T & \\ddots & \\ddots &    \\\\\n       & \\ddots & \\ddots & C \\\\\n       &        &     C^T & B\n \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}\n $$\n mit \n $$\n B = \\begin{bmatrix}\n    4  & -1     &        &    \\\\\n    -1 & \\ddots & \\ddots &    \\\\\n       & \\ddots & \\ddots & -1 \\\\\n       &        &     -1 & 4\n    \\end{bmatrix} \\in \\mathbb{R}^{\\sqrt{n}\\times \\sqrt{n}}\n$$\nund\n$$\nC = \\begin{bmatrix}\n    -1  & 0      &        &   \\\\\n    0   & \\ddots & \\ddots &   \\\\\n        & \\ddots & \\ddots &  0\\\\\n    -1  &        &      0 & -1\n    \\end{bmatrix} \\in \\mathbb{R}^{\\sqrt{n}\\times \\sqrt{n}}.\n$$\n\n$A$ ist also eine symmetrische Matrix mit $4$ auf der Diagonalen und $-1$ auf den ersten und $\\sqrt{n}$-ten Nebendiagonalen.\n\nZiel ist es, dieses System mithilfe des **CG-Verfahrens** (konjugierte Gradienten) und des **PCG-Verfahrens** (präkonditionierte Gradienten) zu lösen.",
      "metadata": {}
    },
    {
      "id": "7f50834f-abf8-4340-8070-3184a56bdbae",
      "cell_type": "markdown",
      "source": "Tragen Sie zunächst in der folgenden Zelle **beide** Ihre Namen ein: ",
      "metadata": {}
    },
    {
      "id": "969efe4a-8fbb-404a-8ed4-64110243b90f",
      "cell_type": "code",
      "source": "# Numerische Optimierungsverfahren der Wirtschaftsmathematik\n# Wintersemester 2025/2026\n# Übungsblatt 6 - Programmieraufgabe 6\n#\n# [Nachname], [Vorname]\n# [Vorname.Nachname@uni-a.de]\n# \n# [Nachname], [Vorname]\n# [Vorname.Nachname@uni-a.de]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4858e9a7-3e0e-443a-825b-080ba4ecd579",
      "cell_type": "code",
      "source": "import math\nimport time\nimport numpy as np\nfrom numpy.linalg import norm\nfrom numpy.linalg import inv\nimport matplotlib.pyplot as plt\nimport sys",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6a07d1cd-c451-4a3f-a121-a01797e349e1",
      "cell_type": "markdown",
      "source": "## Teil 1: Implementierung der Hilfsfunktionen\n\nFür großes $n$ ist es sehr ineffizient, die Matrix $A$ und den Präkonditionierer $S$ zu speichern. Daher wollen wir die Anwendung dieser Matrizen direkt als Funktionen implementieren. Dieser Ansatz wird auch als \"Matrix-Free\" bezeichnet.\n\n* `Av = mul_A(v)`: Diese Funktion soll die Multiplikation der Matrix $A$ mit einem beliebigen Vektor $v$ effizient realisieren, **ohne** dass $A$ als $n \\times n$-Matrix gespeichert werden muss.\n* `w  = mul_pre(r)`: Analog soll diese Funktion die Multiplikation eines Vektors $r$ mit $S^{-T} S^{-1}$ realisieren.\n\nFür die Präkonditionierung soll, wie in der Vorlesung besprochen, $S = \\frac{1}{2} D + L^T$ verwendet werden, wobei die Zerlegung $A = L + D + L^T$ zugrunde liegt ($L$ ist der strikte obere Dreiecksteil und $D$ die Diagonale von $A$). \n\n$S^{-T}S^{-1}$ kann dann leicht mittel Vorwärts- und Rückwärtssubstitution angewendet werden.",
      "metadata": {}
    },
    {
      "id": "d10bfd72-6742-48bf-a2e2-ee6958728dab",
      "cell_type": "code",
      "source": "def mul_A(v):\n    \"\"\"\n    A*v = mul_A(v)\n    Legen Sie hier nicht die Matrix A an!\n    \"\"\"\n    sqrt_n = int(np.sqrt(v.shape[0]))\n    Av = 4.0 * v\n    Av[:-1] -= ???\n    Av[1:] -= ???\n    Av[:-sqrt_n] -= ???\n    Av[sqrt_n:] -= ???\n    return Av",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7a5791eb-65aa-4137-8333-c3fdc4958a55",
      "cell_type": "code",
      "source": "def mul_pre(r):\n    \"\"\"\n    Anwendung des Vorkonditionierers S^(-1)S^(-T) auf r\n    (Auch hier dürfen weder A noch S^(-1) als Matrix angelegt werden)\n    \"\"\"\n    n = r.shape[0]\n    sqrt_n = int(np.sqrt(n))\n    # z = S^(-T)*r\n    z = np.zeros(n)\n    z[0] = r[0] / 2.0\n    for i in range(1, n):\n        z[i] = (r[i] + z[i-1]) / 2.0\n        if i >= int(sqrt_n):\n            z[i] += z[i - sqrt_n] / 2.0\n    # w = S^(-1)*z\n    w = np.zeros(n)\n        ???\n    return w",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "030dd059-9399-4aa1-a9e9-13df1f03207a",
      "cell_type": "markdown",
      "source": "Sie können die folgenden beiden Zeilen zum Testen Ihrer Funktionen verwenden:",
      "metadata": {}
    },
    {
      "id": "1337b1f4-01f7-4b40-8b1e-d8745b0c243e",
      "cell_type": "code",
      "source": "m = 4\nn = 16\nA = 4.0 * np.eye(n) - (np.diag(np.ones(n-1), k=-1) + np.diag(np.ones(n-1), k=1))\nA -= (np.diag(np.ones(n - m), k = m) + np.diag(np.ones(n - m), k = -m))\nx = np.ones(n)\nprint(A @ x)\nprint(mul_A(x))\nassert (A @ x == mul_A(x)).all()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c2bfc99b-0eec-45f7-912e-7cd76ec4536a",
      "cell_type": "code",
      "source": "m = 4\nn = 16\nS =  2.0 * np.eye(n) - np.diag(np.ones(n-1), k=1)\nS -= np.diag(np.ones(n - m), k = m)\nb = np.ones(n)\nprint(inv(S) @ inv(S.T) @ b)\nprint(mul_pre(b))\nassert (inv(S) @ inv(S.T) @ b == mul_pre(b)).all()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d9cba2cd-80c3-436f-ac17-9628787d96bc",
      "cell_type": "markdown",
      "source": "### Teil 2: Implementierung der CG-Verfahren\n\nAls Nächstes werden die Hauptfunktionen für die iterativen Lösungsverfahren benötigt:\n\n* `iterates, gradient_norms = cg(b, mul_A, tol, kmax)`\n* `iterates, gradient_norms = pcg(b, mul_A, mul_pre, tol, kmax)`\n\nDiesen Funktionen werden `mul_A` und `mul_pre` als Parameter übergeben, um die Matrixoperationen durchzuführen.\n\nParameter:\n* `tol` ist die Fehlertoleranz. Die Verfahren sollen abgebrochen werden, wenn **sowohl** die Norm der Gradienten **als auch** die Norm der Differenz aufeinanderfolgender Iterierter $\\leq$ `tol` ist.\n* `kmax` ist die maximale Anzahl an Iterationen. Die Verfahren müssen spätestens terminieren, wenn diese Grenze erreicht ist.\n* Als Startvektor wählen Sie die rechte Seite $b$.\n\n\nDie Rückgabewerte der Funktionen `cg` und `pcg` sollen dabei sein:\n* `iterates` (Vektor): Die vom jeweiligen Verfahren berechneten Iterierten, wobei `iterates[-1,:]` die berechnete Lösung ist.\n* `gradient_norms` (Vektor): Der Vektor mit den Normen der Gradienten $\\|Ax^{(i)} + b\\|_2$.\n",
      "metadata": {}
    },
    {
      "id": "c7bac80a-11d3-4c97-a7b4-3a04ff7c596a",
      "cell_type": "code",
      "source": "def cg(b, mul_A, tol, max_iter):\n    \"\"\"\n    Konjugierte-Gradienten-Verfahren (CG)\n        iterates, gradient_norms = cg(b, mul_A, tol, max_iter)\n\n    Parameter:\n      b        : Rechte Seite und Initialschätzung (Startvektor x0);\n      mul_A    : Funktion, die A*v berechnet;\n      tol      : Geforderte absolute Genauigkeit;\n      max_iter : Maximale Zahl von Iterationen.\n\n    Resultate:\n      iterates       : Folge der Iterierten x^(k)\n      gradient_norms : Folge der Gradientennormen ||A*x^(i) + b||_2;\n\n    Das Verfahren bricht ab, wenn entweder die maximale Zahl von\n    Iterationsschritten 'kmax' erreicht ist oder die 2-Normen des\n    Gradienten (A*x + b) und Schrittweite (x^(i+1) - x^(i) = alpha^(i)*v^(i))\n    beide kleiner-gleich der Toleranz 'tol' sind.\n    \"\"\"\n\n    n = b.shape[0]\n    gradient_norms = np.zeros(max_iter + 1)   # Gradientennorm || A*x^(i) + b ||_2\n\n    # Initialisierung\n    iter_count = 0                            # Zählt die berechneten Iterationsschritte\n    iterates = np.zeros((max_iter + 1, n))\n    iterates[0,:] = np.copy(b)                # x^(0) = b\n    gradient = mul_A(iterates[0,:]) + b       # g^(0) = A*x^(0) + b\n    direction = -1 * np.copy(gradient)        # d^(0) = -g^(0)\n    gamma_old = np.inner(gradient, gradient)  # gamma^(0) = <g^(0), g^(0)>\n    step_norm = np.inf                        # || alpha^(i) * v^(i) ||\n\n    gradient_norms[iter_count] = norm(gradient)\n\n    # Iteration\n    ???\n\n    # Die resultierenden Vektoren sollen genau die richtige Länge haben;\n    # alle überschüssigen Elemente werden abgeschnitten:\n    gradient_norms = gradient_norms[:iter_count + 1]\n    iterates = iterates[:iter_count + 1,:]\n\n    return iterates, gradient_norms",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "56c6b39d-00bf-4816-aa48-63614b3a44b0",
      "cell_type": "code",
      "source": "def pcg(b, mul_A, mul_pre, tol, max_iter):\n    \"\"\"\n    Präkonditioniertes Konjugierte-Gradienten-Verfahren (PCG)\n        iterates, gradient_norms = pcg(b, mul_A, mul_pre, tol, max_iter)\n\n    Parameter:\n      b        : Rechte Seite und Initialschätzung (Startvektor x0);\n      mul_A    : Funktion, die A*v berechnet;\n      mul_pre  : Funktion, die M^(-1)*v berechnet (M^(-1) = S^(-T)*S^(-1));\n      tol      : Geforderte absolute Genauigkeit;\n      max_iter : Maximale Zahl von Iterationen.\n\n    Resultate:\n      iterates  : Folge der Iterierten x^(k)\n      gradient_norms : Folge der Gradientennormen ||A*x^(i) + b||_2;\n\n    Das Verfahren bricht ab, wenn entweder die maximale Zahl von\n    Iterationsschritten 'kmax' erreicht ist oder die 2-Normen des\n    Gradienten (A*x + b) und Schrittweite (x^(i+1) - x^(i) = alpha^(i)*v^(i))\n    beide kleiner-gleich der Toleranz 'tol' sind.\n    \"\"\"\n    \n    n = b.shape[0]\n    gradient_norms = np.zeros(max_iter + 1)               # Gradientennorm || A*x^(i) + b ||_2\n\n    # Initialisierung\n    iter_count = 0                                        # Zählt die berechneten Iterationsschritte\n    iterates = np.zeros((max_iter + 1, n))\n    iterates[0,:] = np.copy(b)                            # x^(0) = b\n    gradient = mul_A(iterates[0,:]) + b                   # g^(0) = A*x^(0) + b\n    preconditioned_gradient =  mul_pre(gradient)          # w_0 = M^(-1) * g_0\n    direction = -1 * np.copy(preconditioned_gradient)     # p_0 = -w_0\n    gamma_old = np.dot(preconditioned_gradient, gradient) # gamma_0 = <w_0, g_0>\n    step_norm = np.inf                                    # || alpha^(i) * v^(i) ||\n\n    gradient_norms[iter_count] = norm(gradient)\n\n    # Iteration\n    ???\n\n    # Die resultierenden Vektoren sollen genau die richtige Länge haben;\n    # alle überschuessigen Elemente werden daher abgeschnitten:\n    gradient_norms = gradient_norms[:iter_count + 1]\n    iterates = iterates[:iter_count + 1,:]\n\n    return iterates, gradient_norms",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6c54f3c6-3417-420d-b98e-13e1ca0a5fc0",
      "cell_type": "markdown",
      "source": "Sie können die folgenden beiden Zeilen zum Testen Ihrer Funktionen verwenden. Dabei sollte `iterates.shape` die Form `(anzahl_iterationen, 25)` und `gradient_norms.shape` die Form `(anzahl_iterationen,)` haben.",
      "metadata": {}
    },
    {
      "id": "364d7eb7-3668-4ffe-b46f-44083ad17b1f",
      "cell_type": "code",
      "source": "iterates, gradient_norms = cg(np.ones(5*5), mul_A, 1e-5, 20)\nprint(iterates.shape)\nprint(gradient_norms.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "68134f35-2185-445e-b2d5-b3ab6c125711",
      "cell_type": "code",
      "source": "iterates, gradient_norms = pcg(np.ones(5*5), mul_A, mul_pre, 1e-5, 20)\nprint(iterates.shape)\nprint(gradient_norms.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "91a4622f-d622-49ab-b4df-942eaadaaaf6",
      "cell_type": "markdown",
      "source": "### Teil 3: Vergleich der Verfahren\n\nNun wollen wir die implementierten Verfahren vergleichen. Dazu wenden wir die Funktionen `cg` und `pcg` auf eine Matrix $A \\in \\mathbb{R}^{n \\times n}$ an, wobei $n = m^2$ für ein $m \\in \\mathbb{N}$.",
      "metadata": {}
    },
    {
      "id": "0c144d3e-8d5e-4ce3-81b8-90ccf10f9bb3",
      "cell_type": "code",
      "source": "from util.plotting_06 import compute_and_plot_06\n\nmaxit = 5000\ntol = 1e-15\nm = 50\n\nb, cg_solution, pcg_solution = compute_and_plot_06(cg, pcg, m, mul_A, mul_pre, tol, maxit)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "576c9143-da24-4d4c-9fcc-89a81e3c4b4d",
      "cell_type": "markdown",
      "source": "Zum Schluss können wir auch noch unsere Lösung der Wärmeleitgleichung plotten:",
      "metadata": {}
    },
    {
      "id": "996dac79-a9af-4b0c-b7d1-edbb05ef1be3",
      "cell_type": "code",
      "source": "plt.figure()\nplt.subplot(121)\nplt.imshow(b.reshape(m,m))\nplt.title(\"$b$\")\nplt.subplot(122)\nplt.imshow(-1 * pcg_solution.reshape(m, m)) # pcg_solution ist argmax 0.5*x^TAx + b^Tx = -A^(-1)b\nplt.title(\"$A^{-1}b$\") \nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3c991232-e6dd-4d12-a532-6b23ed0834c8",
      "cell_type": "markdown",
      "source": "Optionale Zusatzfrage (ohne Bewertung): Wie würde es sich optisch auf das Ergebnis $A^{-1}b$ auswirken, wenn Sie die Matrix $A$ wie in Beispiel 3.39 im Skript, also mit $C = -I$ wählen würden?",
      "metadata": {}
    }
  ]
}