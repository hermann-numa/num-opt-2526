{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "1c2230bf-18c1-40c2-8a6a-5c1d86705948",
      "cell_type": "markdown",
      "source": "# Programmieraufgabe: CG-Verfahren und PCG-Verfahren\n\nWie in Beispiel 3.39 im Skript betrachten wir ein quadratisches Optimierungsproblem, das aus der zweidimensionalen Wärmeleitungsgleichung resultiert. Dabei hat die Matrix $A$ die Form\n$$\n A = \\begin{bmatrix}\n    B  & -I     &        &    \\\\\n    -I & \\ddots & \\ddots &    \\\\\n       & \\ddots & \\ddots & -I \\\\\n       &        &     -I & B\n \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}\n $$\n mit \n $$\n B = \\begin{bmatrix}\n    4  & -1     &        &    \\\\\n    -1 & \\ddots & \\ddots &    \\\\\n       & \\ddots & \\ddots & -1 \\\\\n       &        &     -1 & 4\n    \\end{bmatrix} \\in \\mathbb{R}^{\\sqrt{n}\\times \\sqrt{n}}.\n$$\nZiel ist es, dieses System mithilfe des **CG-Verfahrens** (konjugierte Gradienten) und des **PCG-Verfahrens** (präkonditionierte Gradienten) zu lösen.",
      "metadata": {}
    },
    {
      "id": "7f50834f-abf8-4340-8070-3184a56bdbae",
      "cell_type": "markdown",
      "source": "Tragen Sie zunächst in der folgenden Zelle **beide** Ihre Namen ein: ",
      "metadata": {}
    },
    {
      "id": "969efe4a-8fbb-404a-8ed4-64110243b90f",
      "cell_type": "code",
      "source": "# Numerische Optimierungsverfahren der Wirtschaftsmathematik\n# Wintersemester 2025/2026\n# Übungsblatt 6 - Programmieraufgabe 6\n#\n# [Nachname], [Vorname]\n# [Vorname.Nachname@uni-a.de]\n# \n# [Nachname], [Vorname]\n# [Vorname.Nachname@uni-a.de]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4858e9a7-3e0e-443a-825b-080ba4ecd579",
      "cell_type": "code",
      "source": "import math\nimport time\nimport numpy as np\nfrom numpy.linalg import norm\nfrom numpy.linalg import inv\nimport matplotlib.pyplot as plt\nimport sys",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6a07d1cd-c451-4a3f-a121-a01797e349e1",
      "cell_type": "markdown",
      "source": "## Teil 1: Implementierung der Hilfsfunktionen\n\nFür großes $n$ ist es sehr ineffizient, die Matrix $A$ und den Präkonditionierer $S$ zu speichern. Daher wollen wir die Anwendung dieser Matrizen direkt als Funktionen implementieren. Dieser Ansatz wird auch als \"Matrix-Free\" bezeichnet.\n\n* `Av = mul_A(v)`: Diese Funktion soll die Multiplikation der Matrix $A$ mit einem beliebigen Vektor $v$ effizient realisieren, **ohne** dass $A$ als $n \\times n$-Matrix gespeichert werden muss.\n* `w  = mul_pre(r)`: Analog soll diese Funktion die Multiplikation eines Vektors $r$ mit $S^{-T} S^{-1}$ realisieren.\n\nFür die Präkonditionierung soll, wie in der Vorlesung besprochen, $S = \\frac{1}{2} D + L^T$ verwendet werden, wobei die Zerlegung $A = L + D + L^T$ zugrunde liegt ($L$ ist der strikte obere Dreiecksteil und $D$ die Diagonale von $A$). \n\n$S^{-T}S^{-1}$ kann dann leicht mittel Vorwärts- und Rückwärtssubstitution angewendet werden.",
      "metadata": {}
    },
    {
      "id": "d10bfd72-6742-48bf-a2e2-ee6958728dab",
      "cell_type": "code",
      "source": "def mul_A(v):\n    \"\"\"\n    A*v = mul_A(v)\n    Legen Sie hier nicht die Matrix A an!\n    \"\"\"\n    sqrt_n = int(np.sqrt(v.shape[0]))\n    Av = 4.0 * v\n    Av[:-1] -= v[1:]\n    Av[1:] -= v[:-1]\n    Av[:-sqrt_n] -= v[sqrt_n:]\n    Av[sqrt_n:] -= v[:-sqrt_n]\n    return Av",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7a5791eb-65aa-4137-8333-c3fdc4958a55",
      "cell_type": "code",
      "source": "def mul_pre(r):\n    \"\"\"\n    Anwendung des Vorkonditionierers S^(-1)S^(-T) auf r\n    (Auch hier dürfen weder A noch S^(-1) als Matrix angelegt werden)\n    \"\"\"\n    n = r.shape[0]\n    sqrt_n = int(np.sqrt(n))\n    # z = S^(-T)*r\n    z = np.zeros(n)\n    z[0] = r[0] / 2.0\n    for i in range(1, n):\n        z[i] = (r[i] + z[i-1]) / 2.0\n        if i >= int(sqrt_n):\n            z[i] += z[i - sqrt_n] / 2.0\n    # w = S^(-1)*z\n    w = np.zeros(n)\n    w[-1] = z[-1] / 2.0\n    for i in range(2, n+1):\n        w[-i] = (z[-i] + w[-(i-1)]) / 2.0\n        if i >= int(sqrt_n) + 1:\n            w[-i] += w[-(i - sqrt_n)] / 2.0\n    return w",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "030dd059-9399-4aa1-a9e9-13df1f03207a",
      "cell_type": "markdown",
      "source": "Sie können die folgenden beiden Zeilen zum Testen Ihrer Funktionen verwenden:",
      "metadata": {}
    },
    {
      "id": "1337b1f4-01f7-4b40-8b1e-d8745b0c243e",
      "cell_type": "code",
      "source": "m = 4\nn = 16\nA = 4.0 * np.eye(n) - (np.diag(np.ones(n-1), k=-1) + np.diag(np.ones(n-1), k=1))\nA -= (np.diag(np.ones(n - m), k = m) + np.diag(np.ones(n - m), k = -m))\nx = np.ones(n)\nprint(A @ x)\nprint(mul_A(x))\nassert (A @ x == mul_A(x)).all()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c2bfc99b-0eec-45f7-912e-7cd76ec4536a",
      "cell_type": "code",
      "source": "m = 4\nn = 16\nS =  2.0 * np.eye(n) - np.diag(np.ones(n-1), k=1)\nS -= np.diag(np.ones(n - m), k = m)\nb = np.ones(n)\nprint(inv(S) @ inv(S.T) @ b)\nprint(mul_pre(b))\nassert (inv(S) @ inv(S.T) @ b == mul_pre(b)).all()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d9cba2cd-80c3-436f-ac17-9628787d96bc",
      "cell_type": "markdown",
      "source": "### Teil 2: Implementierung der CG-Verfahren\n\nAls Nächstes werden die Hauptfunktionen für die iterativen Lösungsverfahren benötigt:\n\n* `iterates, residuals = cg(b, mul_A, tol, kmax)`\n* `iterates, residuals = pcg(b, mul_A, mul_pre, tol, kmax)`\n\nDiesen Funktionen werden `mul_A` und `mul_pre` als Parameter übergeben, um die Matrixoperationen durchzuführen.\n\nParameter:\n* `tol` ist die Fehlertoleranz. Die Verfahren sollen abgebrochen werden, wenn **sowohl** die Norm des Residuums **als auch** die Norm der Differenz aufeinanderfolgender Iterierter $\\leq$ `tol` ist.\n* `kmax` ist die maximale Anzahl an Iterationen. Die Verfahren müssen spätestens terminieren, wenn diese Grenze erreicht ist.\n* Als Startvektor wählen Sie die rechte Seite $b$.\n\n\nDie Rückgabewerte der Funktionen `cg` und `pcg` sollen dabei sein:\n* `iterates`: Die vom jeweiligen Verfahren berechneten Iterierten, wobei `iterates[-1,:]` die berechnete Lösung ist.\n* `residuals` (Vektor): Der Vektor mit den Normen des Residuums:\n    $$\\text{res}(i) = \\| b - A x^{(i)} \\|_{2}$$\n",
      "metadata": {}
    },
    {
      "id": "c7bac80a-11d3-4c97-a7b4-3a04ff7c596a",
      "cell_type": "code",
      "source": "def cg(b, mul_A, tol, max_iter):\n    \"\"\"\n    Konjugierte-Gradienten-Verfahren (CG)\n        iterates, residuals = cg(b, mul_A, tol, max_iter)\n\n    Parameter:\n      b        : Rechte Seite und Initialschätzung (Startvektor x0);\n      mul_A    : Funktion, die A*v berechnet;\n      tol      : Geforderte absolute Genauigkeit;\n      max_iter : Maximale Zahl von Iterationen.\n\n    Resultate:\n      iterates  : Folge der Iterierten x^(k)\n      residuals : Folge der Residuennormen ||b - A*x^(i)||_2;\n\n    Das Verfahren bricht ab, wenn entweder die maximale Zahl von\n    Iterationsschritten 'kmax' erreicht ist oder die 2-Normen von\n    Residuum (A*x - b) und Schrittweite (x^(i+1) - x^(i)) beide kleiner-gleich\n    der Toleranz 'tol' sind.\n    \"\"\"\n    n = b.shape[0]\n\n    residuals = np.zeros(max_iter + 1)  # Residuum  || A*x^(i) - b ||_2\n    \n    # Initialisierung\n    iter_count = 0                           # Zählt die berechneten Iterationsschritte\n    iterates = np.zeros((max_iter + 1, n))\n    iterates[0,:] = np.copy(b)               # x_0 = b\n    residual = b - mul_A(iterates[0,:])           # r_0 = b - A*x_0\n    search_direction = np.copy(residual)     # p_0 = r_0\n    gamma_old = np.inner(residual, residual) # gamma_0 = <r_0, r_0>\n\n    residuals[iter_count] = math.sqrt(gamma_old) \n\n    while (iter_count < max_iter) and (\n        (residuals[iter_count] > tol) or (step_norm > tol)\n    ):\n        iter_count += 1 \n        # 1. Berechne A*p_k\n        v = mul_A(search_direction)\n        \n        # 2. Berechne alpha_k (Schrittweite)\n        omega = np.inner(search_direction, v) # <p_k, A*p_k>\n        \n        #if abs(omega) < 1e-14:\n        #    print(\n        #        f\"CG-Warnung: k={iter_count}/{max_iter} , gamma_old={format(gamma_old,'.2e')} , <p_k,v_k>={format(omega,'.2e')} < 1e-14 !!\"\n        #    )\n        #    break\n\n        alpha = gamma_old / omega # alpha_k = gamma_k / omega_k\n\n        # 3. Update x_k+1\n        step_vector = alpha * search_direction\n        step_norm = norm(step_vector)\n        iterates[iter_count,:] = iterates[iter_count - 1,:] + step_vector\n\n        # 4. Update r_k+1\n        residual -= alpha * v\n\n        # 5. Berechne gamma_k+1\n        gamma_new = np.inner(residual, residual) # gamma_k+1 = <r_k+1, r_k+1>\n\n        # 6. Update p_k+1\n        beta = gamma_new / gamma_old # beta_k = gamma_k+1 / gamma_k\n        \n        search_direction *= beta\n        search_direction += residual\n        \n        # 7. Speichern und Update\n        \n        # Residuen-Norm für den Abbruch: ||r_k+1||_2\n        residuals[iter_count] = norm(residual)\n        \n        gamma_old = gamma_new\n\n    # Die resultierenden Vektoren sollen genau die richtige Länge haben;\n    # alle überschüssigen Elemente werden abgeschnitten:\n    residuals = residuals[: iter_count + 1]\n    iterates = iterates[:iter_count + 1,:]\n\n    return iterates, residuals",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "56c6b39d-00bf-4816-aa48-63614b3a44b0",
      "cell_type": "code",
      "source": "def pcg(b, mul_A, mul_pre, tol, max_iter):\n    \"\"\"\n    Präkonditioniertes Konjugierte-Gradienten-Verfahren (PCG)\n        iterates, residuals = pcg(b, mul_A, mul_pre, tol, max_iter)\n\n    Parameter:\n      b        : Rechte Seite und Initialschätzung (Startvektor x0);\n      mul_A    : Funktion, die A*v berechnet;\n      mul_pre  : Funktion, die M^(-1)*v berechnet (M^(-1) = S^(-T)*S^(-1));\n      tol      : Geforderte absolute Genauigkeit;\n      max_iter : Maximale Zahl von Iterationen.\n\n    Resultate:\n      iterates  : Folge der Iterierten x^(k)\n      residuals : Folge der Residuennormen ||b - A*x^(i)||_2;\n\n    Das Verfahren bricht ab, wenn entweder die maximale Zahl von\n    Iterationsschritten 'kmax' erreicht ist oder die 2-Normen von\n    Residuum (A*x - b) und Schrittweite (x^(i+1) - x^(i)) beide kleiner-gleich\n    der Toleranz 'tol' sind.\n    \"\"\"\n    n = b.shape[0]\n\n    # Initialisierung der Ergebnis-Vektoren (mit Platz für den Startwert, Index 0)\n    residuals = np.zeros(max_iter + 1)  # Residuum  || A*x^(i) - b ||_2\n\n    # Initialisierung\n    iter_count = 0                     # Zählt die berechneten Iterationsschritte\n    iterates = np.zeros((max_iter + 1, n))\n    iterates[0,:] = np.copy(b)           # x_0 = b\n    residual = b - mul_A(iterates[0,:])       # r_0 = b - A*x_0\n    \n    # Präkonditionierung: w_0 = M^(-1) * r_0\n    preconditioned_residual = mul_pre(residual) # w_0 = M^(-1) * r_0\n    \n    # Suchrichtung: p_0 = w_0\n    search_direction = np.copy(preconditioned_residual) \n    \n    # Skalarprodukt: gamma_0 = <w_0, r_0>\n    gamma_old = np.dot(preconditioned_residual, residual) \n\n    # Speichern der Werte für Iteration k=0\n    residuals[iter_count] = norm(residual) \n\n    # Iteration\n    while (iter_count < max_iter) and (\n        (residuals[iter_count] > tol) or (step_norm > tol)\n    ):\n        iter_count += 1\n        # 1. Berechne A*p_k\n        vec_v = mul_A(search_direction)\n        \n        # 2. Berechne alpha_k (Schrittweite)\n        # omega = <p_k, A*p_k>\n        omega = np.dot(search_direction, vec_v) \n        \n        alpha = gamma_old / omega # alpha_k = gamma_k / omega_k\n\n        # 3. Update x_k+1\n        step_vector = alpha * search_direction\n        step_norm = norm(step_vector)\n        iterates[iter_count,:] = iterates[iter_count - 1,:] + step_vector\n\n        # 4. Update r_k+1\n        residual -= alpha * vec_v\n        \n        # 5. Präkonditionierung: w_k+1 = M^(-1) * r_k+1\n        preconditioned_residual = mul_pre(residual)\n        \n        # 6. Berechne gamma_k+1mul\n        gamma_new = np.dot(preconditioned_residual, residual) # gamma_k+1 = <w_k+1, r_k+1>\n\n        # 7. Update p_k+1\n        beta = gamma_new / gamma_old # beta_k = gamma_k+1 / gamma_k\n        \n        search_direction *= beta\n        search_direction += preconditioned_residual\n        \n\n        # Residuen-Norm für den Abbruch: ||r_k+1||_2\n        residuals[iter_count] = norm(residual)\n        \n        gamma_old = gamma_new\n\n    # Die resultierenden Vektoren sollen genau die richtige Länge haben;\n    # alle überschuessigen Elemente werden daher abgeschnitten:\n    residuals = residuals[:iter_count + 1]\n    iterates = iterates[:iter_count + 1,:]\n\n    return iterates, residuals",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6c54f3c6-3417-420d-b98e-13e1ca0a5fc0",
      "cell_type": "markdown",
      "source": "Sie können die folgenden beiden Zeilen zum Testen Ihrer Funktionen verwenden. Dabei sollte `iterates.shape` die Form `(anzahl_iterationen, 25)` und `residuals.shape` die Form `(anzahl_iterationen,)` haben.",
      "metadata": {}
    },
    {
      "id": "364d7eb7-3668-4ffe-b46f-44083ad17b1f",
      "cell_type": "code",
      "source": "iterates, residuals = cg(np.ones(5*5), mul_A, 1e-5, 20)\nprint(iterates.shape)\nprint(residuals.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "68134f35-2185-445e-b2d5-b3ab6c125711",
      "cell_type": "code",
      "source": "iterates, residuals = pcg(np.ones(5*5), mul_A, mul_pre, 1e-5, 20)\nprint(iterates.shape)\nprint(residuals.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "91a4622f-d622-49ab-b4df-942eaadaaaf6",
      "cell_type": "markdown",
      "source": "### Teil 3: Vergleich der Verfahren\n\nNun wollen wir die implementierten Verfahren vergleichen. Dazu wenden wir die Funktionen `cg` und `pcg` auf eine Matrix $A \\in \\mathbb{R}^{n \\times n}$ an, wobei $n = m^2$ für ein $m \\in \\mathbb{N}$.",
      "metadata": {}
    },
    {
      "id": "0c144d3e-8d5e-4ce3-81b8-90ccf10f9bb3",
      "cell_type": "code",
      "source": "from util.plotting_06 import compute_and_plot_06\n\nmaxit = 5000\ntol = 1e-15\nm = 50\n\nb, cg_solution, pcg_solution = compute_and_plot_06(cg, pcg, m, mul_A, mul_pre, tol, maxit)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "576c9143-da24-4d4c-9fcc-89a81e3c4b4d",
      "cell_type": "markdown",
      "source": "Zum Schluss können wir auch noch unsere Lösung der Wärmeleitgleichung plotten:",
      "metadata": {}
    },
    {
      "id": "996dac79-a9af-4b0c-b7d1-edbb05ef1be3",
      "cell_type": "code",
      "source": "plt.figure()\nplt.subplot(121)\nplt.imshow(b.reshape(m,m))\nplt.title(\"$b$\")\nplt.subplot(122)\nplt.imshow(pcg_solution.reshape(m, m))\nplt.title(\"$A^{-1}b$\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}